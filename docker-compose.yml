services:
  backend:
    build:
      context: ./backend
      dockerfile: Dockerfile
    container_name: ${SESSION_ID}_backend
    hostname: backend
    environment:
      # Session configuration
      - SESSION_ID=${SESSION_ID}
      - SECRET_KEY=${SECRET_KEY}

      # API Configuration
      - ALLOWED_ORIGINS=http://localhost:${FRONTEND_PORT},http://127.0.0.1:${FRONTEND_PORT}

      # Storage paths (tmpfs mounted)
      - UPLOAD_DIR=/tmp/sandbox/uploads

      # LLM Configuration (for future integration)
      - OLLAMA_BASE_URL=http://ollama:11434
      - DEFAULT_MODEL=deepseek-r1:8b

    ports:
      - "${BACKEND_PORT}:8000"

    # Ephemeral tmpfs storage - no persistent data
    tmpfs:
      - /tmp/sandbox:size=10G,noexec,nosuid,uid=1000,gid=1000
      - /tmp:size=1G,noexec,nosuid

    # Health check
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s

    # Network isolation
    networks:
      - session_network

    # Dependencies
    depends_on:
      ollama:
        condition: service_healthy

    # Session-specific volumes for configuration
    volumes:
      - ./deployment/sessions/${SESSION_ID}/session.json:/app/sessions.json:ro

    # Automatic restart policy
    restart: unless-stopped

  frontend:
    build:
      context: ./frontend
      dockerfile: Dockerfile
      args:
        NEXT_PUBLIC_API_URL: http://localhost:${BACKEND_PORT}
    container_name: ${SESSION_ID}_frontend
    hostname: frontend
    environment:
      # API endpoint configuration for browser access
      - NEXT_PUBLIC_API_URL=http://localhost:${BACKEND_PORT}
      # Internal API URL for server-side requests (Docker network)
      - NEXT_PUBLIC_SESSION_ID=${SESSION_ID}

    ports:
      - "${FRONTEND_PORT}:3000"

    # Health check
    healthcheck:
      test:
        [
          "CMD",
          "wget",
          "--no-verbose",
          "--tries=1",
          "--spider",
          "http://localhost:3000/",
        ]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s

    # Network isolation
    networks:
      - session_network

    # Depends on backend
    depends_on:
      backend:
        condition: service_healthy

    # Automatic restart policy
    restart: unless-stopped

  # Ollama service for LLM capabilities
  ollama:
    image: ollama/ollama:latest
    container_name: ${SESSION_ID}_ollama
    hostname: ollama
    environment:
      - OLLAMA_KEEP_ALIVE=24h
      - DEFAULT_MODEL=${DEFAULT_MODEL:-deepseek-r1:8b}
      - OLLAMA_MODELS=${OLLAMA_MODELS:-}

    # Custom entrypoint to auto-pull models
    entrypoint: ["/scripts/ollama_init.sh"]

    # Volumes including init script
    volumes:
      - ollama_models:/root/.ollama
      - ./deployment/scripts/ollama_init.sh:/scripts/ollama_init.sh:ro

    # Health check for model availability
    healthcheck:
      test: ["CMD", "ollama", "list"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s

    # Network isolation
    networks:
      - session_network

    restart: unless-stopped

# Session-specific network for complete isolation
networks:
  session_network:
    driver: bridge
    name: ${SESSION_ID}_network
    ipam:
      config:
        - subnet: ${SESSION_SUBNET:-172.20.0.0/24}

# Named volumes (only for ollama models)
volumes:
  ollama_models:
    name: ${SESSION_ID}_models
